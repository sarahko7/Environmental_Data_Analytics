---
title: "Assignment 3: Data Exploration"
author: "Sarah Ko"
output: pdf_document
geometry: margin=2.54cm
---

## OVERVIEW

This exercise accompanies the lessons in Environmental Data Analytics (ENV872L) on data exploration. 

## Directions
1. Change "Student Name" on line 3 (above) with your name.
2. Use the lesson as a guide. It contains code that can be modified to complete the assignment.
3. Work through the steps, **creating code and output** that fulfill each instruction.
4. Be sure to **answer the questions** in this assignment document.
Space for your answers is provided in this document and is indicated by the ">" character.
If you need a second paragraph be sure to start the first line with ">".
You should notice that the answer is highlighted in green by RStudio. 
6. When you have completed the assignment, **Knit** the text and code into a single PDF file.
You will need to have the correct software installed to do this (see Software Installation Guide)
Press the `Knit` button in the RStudio scripting panel.
This will save the PDF output in your Assignments folder.
8. After Knitting, please submit the completed exercise (PDF file) to the dropbox in Sakai. Please add your last name into the file name (e.g., "Salk_A02_DataExploration.pdf") prior to submission.

The completed exercise is due on Thursday, 31 January, 2019 before class begins.

## 1) Set up your R session

Check your working directory, load necessary packages (tidyverse), and upload the North Temperate Lakes long term monitoring dataset for the light, temperature, and oxygen data for three lakes (file name: NTL-LTER_Lake_ChemistryPhysics_Raw.csv). Type your code into the R chunk below.
```{r}

# check the working directory
getwd()

# Load necessary package 'tidyverse'
library(tidyverse)

# set wd to the filepath of Environmental_Data_Analytics/Assignments to use relative filepath
# upload the North Temperate Lakes long term monitoring dataset using relative filepath
NTL.data <- read.csv("../Data/Raw/NTL-LTER_Lake_ChemistryPhysics_Raw.csv")

```

## 2) Learn about your system

Read about your dataset in the NTL-LTER README file. What are three salient pieces of information you gained from reading this file?

> ANSWER: 
1. The data contains information on Physical and Chemical Limnology, from the timespan of 1984 - 2016
2. This data was collected from 1 central station near the deepest point of each lake
3. When chemical measurements were made in vertical profiles, depths for sampling usually correspond to the surface plus depths of 50%, 25%, 10%, 5% and 1% of the surface irradiance

## 3) Obtain basic summaries of your data

Write R commands to display the following information: 

1. dimensions of the dataset
2. class of the dataset
3. first 8 rows of the dataset
4. class of the variables lakename, sampledate, depth, and temperature
5. summary of lakename, depth, and temperature

```{r}
# 1
dim(NTL.data)

# 2
class(NTL.data)

# 3
head(NTL.data, 8)

# 4
class(NTL.data$lakename)
class(NTL.data$sampledate)
class(NTL.data$depth)
class(NTL.data$temperature_C)

# 5
summary(NTL.data$lakename)
summary(NTL.data$depth)
summary(NTL.data$temperature_C)

```

Change sampledate to class = date. After doing this, write an R command to display that the class of sammpledate is indeed date. Write another R command to show the first 10 rows of the date column. 

```{r}

# change class of sampledate from factor to date. must define original format of data
NTL.data$sampledate <- as.Date(NTL.data$sampledate, format = "%m/%d/%y")

# confirm that the class is date
class(NTL.data$sampledate)

#display the first 10 rows of the date column
head(NTL.data$sampledate, 10)

```

Question: Do you want to remove NAs from this dataset? Why or why not?

> ANSWER: Removing or keeping NAs depends on the research question. If I suspected that the data is unavailable due to another variable, it would be good to keep NAs in the dataset - deleting them may create bias. If I suspected that this data was unavailable at random and I had adequate amounts of data to create my model, I may be inclined to delete the NAs. Another reason to delete the NAs is if the data was unavailable for a predictable reason (e.g. incomplete years like the beginning and ending years of a dataset may have missing data). 

> for this particular dataset, I would want to remove the NAs in order to perform statistical analyses.

## 4) Explore your data graphically

Write R commands to display graphs depicting: 

1. Bar chart of temperature counts for each lake
2. Histogram of count distributions of temperature (all temp measurements together)
3. Change histogram from 2 to have a different number or width of bins
4. Frequency polygon of temperature for each lake. Choose different colors for each lake.
5. Boxplot of temperature for each lake
6. Boxplot of temperature based on depth, with depth divided into 0.25 m increments
7. Scatterplot of temperature by depth

```{r}

# create dataframe with only lakename and temperature data
NTL.lakename.temp <- NTL.data[,c("lakename", "temperature_C")]

# 1

# remove NAs
NTL.lakename.temp.complete <- na.omit(NTL.lakename.temp)

# barplot counts the occurance of each lake, which each has a corresponding temperature record
Q1.barplot <- ggplot(data = NTL.lakename.temp.complete, aes(x = lakename)) +
    geom_bar()
  print(Q1.barplot)
#SKO: is there an easier way? https://stackoverflow.com/questions/24895575/ggplot2-bar-plot-with-two-categorical-variables
    
# 2
  
# histogram of temp
ggplot(NTL.lakename.temp.complete) +
  geom_histogram(aes(x = temperature_C))

# 3

# create histogram with 50 bins
ggplot(NTL.lakename.temp.complete) +
  geom_histogram(aes(x = temperature_C), bins=50)

# 4

# frequency polygon of temp
ggplot(NTL.lakename.temp.complete) +
  geom_freqpoly(aes(x = temperature_C, color = lakename), bins = 50) +
  scale_x_continuous(limits = c(0, 30)) +
  theme(legend.position = "right")

# 5

# boxplot of temp per lake
ggplot(NTL.lakename.temp.complete) +
  geom_boxplot(aes(x = lakename, y = temperature_C, group = cut_width(lakename, 1)))

# 6

# create dataframe with only lakename, temp, and depth
NTL.lakename.temp.depth <- NTL.data[,c("lakename", "temperature_C", "depth")]

# remove NAs
NTL.lakename.temp.depth.complete <- na.omit(NTL.lakename.temp.depth)

# boxplot of temp based on depth, depth divided into 0.25m increments
ggplot(NTL.lakename.temp.depth.complete) +
  geom_boxplot(aes(x = depth, y = temperature_C, group = cut_width(depth, 0.25)))
#SKO: how to tell the units of this dataset? couldn't find in readme or website

# 7 

# scatterplot temp based on depth
ggplot(NTL.lakename.temp.depth.complete) +
  geom_point(aes(x = depth, y = temperature_C))

```
## 5) Form questions for further data analysis

What did you find out about your data from the basic summaries and graphs you made? Describe in 4-6 sentences.

> ANSWER: The bar plot of temperature readings showed that Paul Lake and Peter Lake have by far the most temperature measurements, followed by Tuesday Lake, West Long Lake, and East Long Lake. Temperature measurements compiled from all the lakes show a range of 3-28 C. Most of the temperature measurements are around 5 C, with a small cluster of measurements around 23 C. At shallower depths (~0-7m), the temperature measurements have a much larger distribution as compared to those deeper. Even so, the temperature appears to decrease with increasing depth below the surface.

> SKO: need to confirm that depths are in meters

What are 3 further questions you might ask as you move forward with analysis of this dataset?

> ANSWER 1: What is the mathematical correlation between temperature and depth?

> ANSWER 2: Are the temperature means of the lakes significantly different from one another?

> ANSWER 3: Is there a correlation between temperature and number of temperature measurements recorded?
